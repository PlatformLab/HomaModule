This file contains various notes and lessons learned concerning performance
of the Homa Linux kernel module.  The notes are in reverse chronological
order: most recent notes first.

* (May 2020) Noticed that cores can disappear for 10-12ms, during which
  softirq handlers do not get invoked. Homa timetraces show no activity
  of any kind during that time (e.g., no interrupts either).

* (Mar. 2020) For the slowdown tests, the --port-max value needs to be
  pretty high to get true Poisson behavior. It was originally 20, but
  increasing it had significant impact on performance for TCP, particularly
  for short-message workloads. For example, TCP P99 slowdown for W1 increased
  from 15 to 170x when --port-max increase from 20-100. Performance
  got even worse at --port-max=200, but I decided to stick with 100 for now.

* (Mar. 2020) Having multiple threads receiving on a single port makes a
  big difference in tail latency. cperf had been using just one receiver
  thread for each port (on both clients and servers); changing to
  multiple threads reduced P50/P99 slowdown for small messages in W5
  from 7/65 to 2.5/7.5!

* Performance suffers from a variety of load balancing problems. Here
  are some examples:
  * (March 2020) Throughput varies by 20% from run to run when a single client
    sends 500KB messages to a single server. In this configuration, all
    packets arrive through a single NAPI core, which is fully utilized.
    However, if Linux also happens to place other threads on that core (such
    as the pacer) it takes time away from NAPI, which reduces throughput.
  * (March 2020) When analyzing tail latency for small messages in W5, I found
    that user threads are occasionally delayed 100s of microseconds in waking
    up to handle a message. The problem is that both the NAPI and SoftIRQ
    threads happened (randomly) to get busy on that core the same time,
    and they completely monopolized the core.
  * (March 2020) Linux switches threads between cores very frequently when
    threads sleep (2/3 of the time in experiments today). 

* (Feb. 2020) The pacer can potentially be a severe performance bottleneck
  (a single thread cannot keep the network utilized with packets that are
  not huge). In a test with 2 clients bombarding a single server with
  1000-byte packets, performance started off high but then suddenly dropped
  by 10x. There were two contributing factors. First, once the pacer got
  involved, all transmissions had to go through the pacer, and the pacer
  became the bottleneck. Second, this resulted in growth of the throttle
  queue (essentially all standing requests: > 300 entries in this experiment).
  Since the queue is scanned from highest to lowest priority, every insertion
  had to scan the entire queue, which took about 6 us. At this point the queue
  lock becomes the bottleneck, resulting in 10x drop in performance.

  I tried inserting RPCs from the other end of the throttle queue, but
  this still left a 2x reduction in throughput because the pacer couldn't
  keep up. In addition, it seems like there could potentially be situations
  where inserting from the other end results in long searches. So, I backed
  this out.

  The solution was to allow threads other than the pacer to transmit packets
  even if there are entries on the throttle queue, as long as the NIC queue
  isn't long. This allows other threads besides the pacer to transmit
  packets if the pacer can't keep up. In order to avoid pacer starvation,
  the pacer uses a modified approach: if the NIC queue is too full for it to
  transmit a packet immediately, it computes the time when it expects the
  NIC queue to get below threshold, waits until that time arrives, and
  then transmits; it doesn't check again to see if the NIC queue is
  actually below threshold (which it may not be if other threads have
  also been transmitting). This guarantees that the pacer will make progress.

* The socket lock is a throughput bottleneck when a multi-threaded server
  is receiving large numbers of small requests. One problem was that the
  lock was being acquired twice while processing a single-packet incoming
  request: once during RPC initialization to add the RPC to active_rpcs,
  and again later to add dispatch the RPC to a server thread. Restructured
  the code to do both of these with a single lock acquisition. Also
  cleaned up homa_wait_for_message to reduce the number of times it
  acquires socket locks. This produced the following improvements, measured
  with one server (--port_threads 8) and 3 clients (--workload 100 --alt_client
  --client_threads 20):
  * Throughput increased from 650 kops/sec to 760
  * socket_lock_miss_cycles dropped from 318% to 193%
  * server_lock_miss_cycles dropped from 1.4% to 0.7%

* Impact of load balancing on latency (xl170, 100B RPCs, 11/2019):
                    1 server thread  18 threads  TCP, 1 thread  TCP, 18 threads
No RPS/RFS             16.0 us         16.3 us      20.0 us        25.5 us
RPS/RFS enabled        17.1 us         21.5 us      21.9 us        26.5 us

* It's better to queue a thread waiting for incoming messages at the *front*
  of the list in homa_wait_for_message, rather than the rear. If there is a
  pool of server threads but not enough load to keep them all busy, it's
  better to reuse a few threads rather than spreading work across all of
  them; this produces better cache locality). This approach improves latency
  by 200-500ns at low loads.

* Problem: large messages have no pipelining. For example, copying bytes
  from user space to output buffers is not overlapped with sending packets,
  and copying bytes from buffers to user space doesn't start until the
  entire message has been received.
  * Tried overlapping packet transmission with packet creation (7/2019) but
    this made performance worse, not better. Not sure why.

* Waking up a process after a message arrives is expensive (a few microseconds).
  * Tried polling for messages (7/2019), but this made performance worse
    because the softirq handler runs on the same CPU as the receiving user
    process.

* It is hard for the pacer to keep the uplink fully utilized, because it
  gets descheduled for long periods of time.
  * Tried disabling interrupts while the pacer is running, but this doesn't
    work: if a packet gets sent with interrupts disabled, the interrupts get
    reenabled someplace along the way, which can lead to deadlock. Also,
    the VLAN driver uses "interrupts off" as a signal that it should polling
    mode, which doesn't work.
  * Tried calling homa_pacer_xmit from multiple places; this helps a bit
    (5-10%).
  * Tried making the pacer thread a high-priority real-time thread; this
    actually made things a bit worse.

* There can be a long lag in sending grants. One problem is that Linux
  tries to collect large numbers of buffers before invoking the softirq
  handler; this causes grants to be delayed. Implemented max_gro_skbs to
  limit buffering. However, varying the parameter doesn't seem to affect
  throughput (11/13/2019).

* Without RPS enabled, Homa performance is limited by a single core handling
  all softirq actions. In order for RPS to work well, Homa must implement
  its own hash function for mapping packets to cores (the default IP hasher
  doesn't know about Homa ports, so it considers only the peer IP address.
  However, with RPS, packets can get spread out over too many cores, which
  causes poor latency when there is a single client and the server is
  underloaded.

